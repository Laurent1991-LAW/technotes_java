# 数据库



### 为什么limit偏移量大时查询效率会很低？

建表语句如下，主键id与创建时间均有索引：

```sql
CREATE TABLE account (
  id int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  name varchar(255) DEFAULT NULL COMMENT '账户名',
  balance int(11) DEFAULT NULL COMMENT '余额',
  create_time datetime NOT NULL COMMENT '创建时间',
  update_time datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  
  PRIMARY KEY (id),
  KEY idx_name (name),
  KEY idx_create_time (create_time) // 索引
    
) 
ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

对于语句 `select id,name,balance from account where create_time > '2020-09-19' limit 100000,10`，查询过程如下：

1. 由于数据库引擎为innoDB，所以where查询条件经过创建时间的 二级索引树 的 叶子节点 先获取到 主键id；
2. 回到 主键索引树 获取该行数据（回表过程）；
3. 扫描满 1000000+10 条数据后，抛弃前面的100000条，只取最后10条。



### 对于分页操作时limit偏移量过大如何解决？

- limit a, b 中的a为顺序获取 ( 通过二级索引树获取id再经主键索引树 )，不经过索引，因此如果a很大，则需要类似于遍历的操作到第a条记录，之后再执行往后取b条的操作 ;

- 改善方法一为**如果id为连续的**，则 where id > a limit b —— 利用索引 ;

- 改善方法二：利用如下语句减少回表，因为括号内语句select的主键id可直接通过 创建时间的 二级索引树 叶子节点获取——**无需回表**！

  ```sql
  select  
      acct1.id,
      acct1.name,
      acct1.balance 
  FROM account acct1 
  	INNER JOIN 
  (SELECT a.id FROM account a WHERE a.create_time > '2020-09-19' limit 100000, 10) AS acct2 
  on acct1.id= acct2.id;
  ```

  

### 分表分库操作

减少 数据库服务器 的压力 ;

提高检索效率 ;



**垂直分表**

垂直分表是基于数据库中的"列"进行，某个表字段较多（例如一个大表有100多个字段），可以新建一张扩展表，**将不经常用或字段长度较大的字段**拆分出去到扩展表中

- 更便于开发与维护

- 能避免跨页问题，MySQL底层是通过数据页 存储的，一条记录占用空间过大会导致跨页，造成额外的性能开销

- 数据库 以 行 为单位 将数据加载到内存中，这样表中字段长度较短且访问频率较高，内存能加载更多的数据，命中率更高，减少了磁盘IO，从而提升了数据库性能。 

  

**平行分表** 

水平切分分为**库内分表和分库分表**

水平切分后同一张表会出现在多个数据库/表中，每个库/表的内容不同。几种典型的 数据分片规则为：

​	1、根据数值范围

> - 按照**插入时间**区间
> - 根据ID区间来切分
> - 根据**记录类型**
> - 根据数据冷热性进行切分，高频请求数据与使用较少的历史数据迁移到其他库中



2、 数值取模 ：类似于hash散列的规则，分四个库，则根据id除4取mod



**注意问题：**

- 分表的框架sharding-jdbc、mycat ；
- 在分库分表环境中，由于表中数据同时存在不同数据库中，主键值的自增长将失效，某个分区数据库自生成的ID无法保证全局唯一，因此：
  - 解决办法1：UUID是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于UUID非常长，会占用大量的存储空间； 
  - 解决办法2：借助redis实现一个自增的分布式id；
  - 解决办法3：snowflake算法解决了分布式系统生成全局ID的需求，生成64位的Long型数字，但其对时间机器依赖性极强；
  - 解决办法4：使用全局id发号器框架，如easyIdGenerator；
- 涉及到不同数据库、不同表格的操作，还需进行事务性考虑 ：
  - 解决办法1：使用**分布式事务管理框架seata**（包含两种主要模式-代码非入侵型的AT、入侵型的TCC）；
  - 解决办法2：利用消息队列，生产多个事务消息, 最终一次性消费多个事务消息；
  - 解决办法3：A调用B，若B出错，则通过消息队列通知A回滚，注意消息幂等性问题 ；
- 小语种的学习平台，数据量也没有达到海量，比如记录最多的，评论数量30~40w条



### 读写分离

**基本原理：**

- 让 **主数据库**<u>处理 事务性增、改、删操作（INSERT、UPDATE、DELETE）</u>，而 **从数据库**  <u>处理SELECT查询操作</u>；
- 在主服务器上**修改，数据会同步到从服务器**（通过订阅主数据库的binlog实现），从服务器只能提供读取数据，不能写入，实现备份的同时也实现了数据库性能的优化，以及提升了服务器安全。 

**实现方法：**

1、在代码中**根据select 、insert进行路由分类**，这类方法也是目前生产环境下应用最广泛的，利用mycat中间件，配置好mycat以后，**将数据库请求发送到mycat，会自动帮我们根据 查询、更新路由到 主、从数据库**；

2、 **代理一般介于应用服务器和数据库服务器之间**，代理数据库服务器 接收到 应用服务器 的请求后 根据判断后转发到，后端数据库，有以下代表性的程序。



### 关于sql优化有什么常见策略？

sql优化其实就是数据库操作优化，策略有很多，最基本的肯定是sql语句的优化 :

- 最左执行原则，**把可以 大幅缩小查询范围 的语句 尽可能地写在前面**，where age < 20 and id =100而不是where  id =100 and age < 20 ;
- 在sql查询的时候，**经常要进行多表联查，用join关键字一次性查询到结果**，要优于 嵌套子查询，而且用join的时候也要**注意用小表驱动大表** ;
- 关于sql语句，还**应该利用好索引，注意避免索引失效的情况** ;
- **习惯性地使用explain关键字，看看操作的效果**，比如有没有用上索引，或者查询了多少列才完成的操作，都可用explain出来 ;

这些都是sql语句层面的优化，在数据量非常庞大的情况下，光是语句的优化是不足的，还得考虑更进一步的优化策略，比如 读写分离 甚至 分库分表等操作。



### 如何优化sql语句？

- 建表、字段时选择合适的字段类型；

- 考察索引的使用情况, 必要的时候建立联合、复合索引；

- 多表查询使用join关键字时，小表驱动大表；

- 查询时，select 具体的字段而不是*；

  -----------

- 避免使用select * ，实际业务中往往表字段较多，一次性查询所有列的数据 浪费资源（比如内存、CPU）；

- 使用union all，避免使用union——它带有排重效果：过程需要遍历、排序和比较，更耗时，更消耗cpu资源 ;

- 避免逐条插入数据 —> 多次远程连接数据库，消耗性能 >< 批量插入，比如MybatisPlus里的insertBatch()方法 ;

- 对全表数据进行查询并复制时，按id和时间升序，每次只同步一批数据，这一批数据只有100条记录。每次同步完成之后，保存这100条数据中最大的id和时间，给同步下一批数据的时候用 ;

- 用连接join查询而非子查询 ;

- 控制索引的数量 ;

- **字段默认值不要设置为null**，包含null值的列在 复合搜索时 该列不会过索引 ;

- 根据数据库语句关键字的执行顺序，**先尽可能缩小查询范围**，比如能用where进行范围限定的，就尽可能将其写在前面 ;

- 多使用explain 关键字查询效率 ;



### 索引失效

**索引失效情况很多 :**

1. 模糊查询，%前置导致索引失效 ;
2. 类型错误，如字段类型为varchar，where条件用number ;
3. where语句里出现or可能会导致索引失效——比如A字段有索引，B字段没有索引，用or连接两个条件则会导致失效（比如 name='xxx' or age > 18，name有索引age没有，虽然name可以依靠索引快速定位结果，但age还是得全表扫描，还不如直接一起都全表扫描得结果） ——> 解决办法：给字段A\B添加复合索引 ;
4. 最左前缀原则：当使用复合索引时（如建立A-B-C联合索引，则A，A-B，A-B-C都有索引，但B、C单独则没有），最左侧的字段索引可单独使用，右侧单独使用时会失效；
5. 查询时对字段使用了函数，而索引是只针对字段 ;

6. is null索引会失效，列表里一旦出现null，则会遍历检索，不会走索引（is not null则索引依然有效），所以在**制表时，尽量设置默认值，避免为null的情况** ;

7. 负向查询，比如id not in (1, 2, 3)也会导致索引失效；




### explain关键字如何使用？

**重要参数1——type**

**type：使用的索引类型 -> 全表扫描还是索引扫描？**

- **system：**最优，很少能出现。
- **const：**PK或者unique上的等值查询
- **eq_ref：**PK或者unique上的join查询，等值匹配，对于前表的每一行(row)，后表只有一行命中
- **ref：**非唯一索引，等值匹配，可能有多行命中
- **range：**索引上的<u>范围扫描</u>，例如：between / in / > / < 等
- **index：**索引上的全集扫描，例如：InnoDB的count
- **ALL：**最慢，全表扫描(full table scan)

**索引类型性能排行：**

all < index < range < ref < eq_ref < **const** < system

**备注：**index虽然不是全表扫描，但扫描了所有的索引



**重要参数2——rows**

rows : 查询了多少行数据才查询到结果



### 如何选择数据库的字段类型？

1. 能用数字类型，就不用字符串，因为字符的处理往往比数字要慢。
2. 尽可能使用小的类型，比如：用bit存布尔值，**用tinyint存枚举值等**。
3. 长度固定的字符串字段——比如身份证号码，用char类型。
4. 长度可变的字符串字段，用varchar类型。
5. 金额字段用decimal，避免精度丢失问题。



### 数据库中用户ip地址如何存储最合理？

以整数形式存储，mysql内含转换函数 :

ip格式转为整数 : ` inet_aton('192.168.0.1')`

整数转换为ip格式 : ` inet_ntoa(3232235521)`

# Mybatis / MybatisPlus

### 嵌套查询和嵌套结果的区别

**嵌套查询：**

- 用**两次单独的sql查询语句获取结果**，第一次查询到的信息 将成为 第二次查询的条件，比如先查询学生记录，再根据学生的班级id查询到对应班级 ；
- 第一次查询的结果用结果集映射，映射中学生对应的班级用<association>标签封装：
  - 第二次查询将根据查询到学生的class_id作为条件，在另一个select标签中查询出对应的班级，因此需添加属性column="class_id" select="findClass"

```sql
<select id="findAllStudents" ResultMap="stuMapping"> 
	select * from students
</select>


<ResultMap id="stuMapping" autoMapping="true">
	<id column="student_id" property="studentId">
	<association property="class" javaType="Class"
				 column="class_id" select="findClass"/>
</ResultMap>
```

- 第二次查询

```sql
<select id="findClass" ResultMap="claMapping"> 
	select * from classes where class_id = #{class_id}
</select>

<ResultMap id="claMapping" autoMapping="true">
	<id column="class_id" property="classId">
</ResultMap>
```

**总结：**

嵌套查询会有两次sql语句查询，第一次查询获取的结果，将作为第二次查询的条件传入；



**嵌套结果查询：**

- 仅需要一次sql语句查询，使用join关键字进行多表联查，查询结果集将一次显示多张表的联合信息；
- sql语句相对复杂，但在mapper.xml中仅需要一次映射即可；



### association / collection 标签什么时候使用？

- **两者都出现在ResultMap标签内部**；
- ResultMap结果集映射 顾名思义，就是需要 **将sql联合查询结果 和 实体对象的属性 进行映射**时使用；
- 比如一个班级类，内部会有多名学生，所以会有一个List<?>集合类型的 学生属性，在数据库里体现出来的就是多条 学生信息数据，如何实现两者这种一个部门对多个员工、一个学生对多个班级的映射，就会用上collection标签，内部含ofType属性；
- 反过来看，比如 学生只有一个班级，在学生类里就会有一个班级对象，在数据库里体现出来的就是 学生表里的一条数据，对应班级表里的一条数据，在jdk里体现这种一对一映射关系，就会用上association标签，内部含JavaType属性；



### 如何实现三级餐单（树形结构）数据封装？

- 将表中所有数据查询到Java中，以List<?>格式封装为lists ;
- 创建HashMap，键为parentId，值为List<?> :
  - 遍历lists，containsKey()方法判断该parentId是否存在
    - 存在，直接获取对应的键List，add()追加该对象;
    - 不存在，创建ArrayList，add()添加该对象;
- 从HashMap中，取parentId=0的List集合，即为一级目录;
- 遍历一级目录，逐个取其id，从HashMap中使 键等于该id，获取各个一级对象子级List，利用对象的setChildren方法设为子级 ;
- 相同逻辑获取三级目录设为对应二级目录子级。



### 如何实现分页操作？

**分页对象包括 :** 

- 搜索关键词query
- 页数pageNum
- 每页显示数目pageSize
- 结果总数total
- 查询结果rows（Object类型，多为List<?>）



**策略一 : sql语句实现**

- service层实现 : 

  limit参数获取

  参数一: start = （pageNum - 1）* pageSize

  参数二: end = pageSize

- mapper层实现 :

  ```
  List<User> findUsersByPages(@Param("query") String query,
  
                                  @Param("start")Integer start,
  
                                  @Param("end")Integer end);
  ```

- mapper.xml文件 : 动态sql

  ```
  	select * from table 
  
      	<if test  != null and test != ''>
  
      	where title like "%"#{query}"%" 
  
      	</if>
  
      		limit #{start}, #{end};
  ```

  

**策略二 : MybatisPlus的Ipage对象**

- service层实现 :
  - Ipage ipage = new Page<>(pageNum，pageSize);
  - 条件构造器 : queryWrapper.like(flag, "title", 关键字);
  - Mapper.selectPage(ipage, wrapper); 



**策略三:Mybatis的PageHelper** 

- 将需要分页的list作为参数，传给分页插件里面的PageInfo 

` https://blog.csdn.net/nanhuaibeian/article/details/106649936?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165236809416782391812808%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165236809416782391812808&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-106649936-null-null.142^v9^control,157^v4^control&utm_term=pageHelper&spm=1018.2226.3001.4187`



### 动态sql语句是什么？

**目的 :** 数据库的查询或更新操作中，**多个查询条件 或 更新字段 用户可能只填写其中几个，传入时未填写的值为null ==> 这些null不应该成为 查询或更新 的语句**，因此需要动态的sql语句

- <where><if></if> <if></if> </where> 可以去掉多余的and或or
- <set><if></if> <if></if> </set> 可以去掉多余的，
- <choose><when></when><otherwise></otherwise></choose> when与otherwise条件二选一



# Redis

### 

### 简介

Redis在以前的版本中是单线程的，而在6.0后对Redis的io模型做了优化，io Thread为多线程的，但是worker Thread仍然是单线程。 



### Redis使用场景

- 缓存

- 分布式session —— 单点登录系统 ： 单点登录中我们需要随机生成一个token作为key，将用户的信息转为json串作为value保存在redis中，同时设置一定的超时时间

- 限流 : 次数统计，用户IP作为key，可投三次票，每次incr递增，满3次则返回false ;

- 次数统计 : 

  - 点赞
    - 若不需要记录点赞用户信息，直接使用incr即可
    - sadd / srem 微博id 用户id  —— 点赞、取消点赞
    - sismember  微博id 用户id —— 是否点赞
    - smembers 微博id —— 点赞的所有用户
    - scard 微博id —— 点赞数
  - 打卡
  - 签到

- 分布式锁

- 生成全局ID



### Redis指令数据类型和主要指令

> String

**常用命令** ：set/get/decr/incr/mget等；

> Hash

**常用命令** ：hget/hset/hgetall等

> List

**常用命令** ：lpush/rpush/lpop/rpop/lrange等；

> Set

**常用命令** ：sadd/spop/smembers/sunion等；

> Sorted Set

**常用命令** ：zadd/zrange/zrem/zcard等；






### Redis高并发高可用递进策略

|         策略          | 缺陷                                                         |
| :-------------------: | :----------------------------------------------------------- |
|       单体架构        | 内存型, 断电记录全部丢失                                     |
|        持久化         | 缓存多为读取服务, QPS高时, 单体架构可能会崩溃                |
|       主从架构        | 主服务器负责 写入 / 读取 ; 从服务器负责读 ; 主服务器实时同步到从服务器 --> 写入过多, 主服务器可能宕机 --> 需手动将从服务器升级为主 |
|       哨兵机制        | 单个哨兵 : 由于网络延迟, 误判主服务器已下线 --> 哨兵集群 : 投票机制 |
|           -           | 一主多从只能提高读性能, 若写入也频繁, 单个主从架构不足以支撑 |
|       分片集群        | 多个主从架构合成一个集群, 数据按分片规则存在不同的节点 ; **每个节点各自存储一部分数据，所有节点数据之和 才是全量数据** ; 制定一个路由规则，对于不同的 key，把它路由到固定一个实例上进行读写 |
| 客户端分片/服务端分片 | 客户端分片 : key 的路由规则 由 客户端维护, 为避免将路由规则耦合到业务代码中, 将路由规则封装成一个模块SDK **><** 服务端分片 : 路由规则不放在客户端来做，而是在客户端和服务端之间增加一个「中间代理层」，数据的路由规则由它维护 ---> 无需与redis集群交互, 仅与proxy交互即可 |

![64011](E:\doc_repo\002-应用\images\64011.png)



客户端分片 :

![6401](E:\doc_repo\002-应用\images\6401.png)



服务端分片 :

![64022](E:\doc_repo\002-应用\images\64022.png)





###持久化 : RDB  / AOF

RDB （redis database）/ AOF (append-only file)

Redis本身是一种内存性数据库，读写快优势所在，但为了防止系统宕机等意外情况，它默认会有自己的数据持久化策略 —— RDB和AOF



**RDB :** 

- **默认开启**的一种数据持久化策略 ;
- 根据配置，定期地把数据**快照存储**到一个dump.rdb文件;
- redis客户端操作时，可以直接输入 save / bgsave 进行同步、异步操作 ;
- 打开redis.conf文件，可以改变属性:
  - 生成快照文件阈值，比如save 60 1000 (60秒内有1000个key以上发生变化, 则生成快照文件)
  - 快照文件是否压缩 ; 
  - 异步持久化存储 报错，是否继续写入数据等 ;



**AOF**:

- 需要在配置文件中开启AOF持久化方式 ;
- 以**追加日志**的形式, 在一个文件中**不断追加redis的一系列操作指令，因此文件会越来越大** ;
- 追加频率分为三种 : no \ always \ everysec ;
- 存在 rewrite 复写的过程 : 
  - 因为redis的key经常会出现过期失效的操作，比如内存中仅有10w条有效数据，AOF文件中则有100w条的操作记录，Redis会**定期针对 这些有效数据 重新构建AOF文件去覆盖旧文件**
  - 也可手动执行rewrite命令, 分别是rewriteaof和bgrewriteaof
- AOF的日志文件的记录可读性非常的高，即使某一时刻有人执行`flushall`清空了所有数据，只需要拿到aof的日志文件，然后把最后一条的flushall给删除掉，就可以恢复数据



**异同** :

- **RDB每次快照会生成不同的dump.rdb文件，AOF则如名字一样，采取追加的方式存储 redis操作指令到AOF文件中 ;**
- 恢复数据方式不同：rdb恢复数据直接把文件加载到内存，而aof是重新去执行一遍日志记录的指令；
- RDB备份的时间间隔比较长，**几分钟备份一次，如果期间宕机，丢失数据量比较大** >< AOF备份一般设置为1秒钟，所以数据丢失的可能性不大 ; 
- 两者对于生产环境下都是必须的，**RDB用于恢复阶段状态的数据 >< AOF保证数据的完整性**



### 主从架构 是什么？

- 主从架构是redis应对高并发的策略，因为**单机redis能够承载的并发量有限，因此可以建立redis集群，采取主从架构的方式来分解压力，实现高并发下的高可用** ;
- 主从架构本身是一种读写分离的策略 : master主节点负责写，并将数据同步到从节点salve，从节点负责读操作 ;
- 默认新启动的三个redis服务角色都为master，通过设置，如`slaveof 172.17.0.2 6379 `配置主从关系 ;
- 如何保证redis集群主从之间数据一致性？
  - Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份 ;
  - Redis增量复制是指Slave初始化后，开始**正常工作时主服务器发生的写操作同步到从服务器**的过程 ;

![640 (2)](E:\doc_repo\002-应用\images\640 (2).png)





### 哨兵机制是什么？

哨兵（Sentinel）是Redis的主从架构模式下，实现高可用性（high availability）的一种机制。
由一个或多个Sentinel实例（instance）组成的Sentinel系统（system）可以**监视任意多个主服务器，以及这些主服务器属下的所有从服务器**，**并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器**，然后由新的主服务器代替已下线的主服务器继续处理命令请求。



### 哨兵机制的原理

1)：每个Sentinel以**每秒钟一次的频率**向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令。

2)：如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值 (这个配置项指定了需要多少失效时间，一个master才会被这个sentinel主观地认为是不可用的。 单位是毫秒，默认为30秒)， 则这个实例会被 Sentinel 标记为主观下线。

3)：如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态。

4)：当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线 。

5)：当主服务器宕机，哨兵可以监控到服务宕机，在从服务器中选举产生一个新的主服务器。 



**如何实现哨兵机制？**

- 创建sentinel配置文件，内部进行个性化配置

  ```
  cat <<EOF > /etc/redis/sentinel.conf 
  sentinel monitor redis6379 172.17.0.2 6379 1
  EOF
  
  //对于sentinel.conf文件中的内容，还可以基于实际需求，进行增强配置
  
  sentinel monitor redis6379 172.17.0.2 6379 1 
  daemonize yes #后台运行
  logfile "/var/log/sentinel_log.log" #运行日志
  sentinel down-after-milliseconds redis6379 30000 #默认30秒
  ```

  

### 如何保证缓存和数据库的一致性？

**逻辑1 :**

**查询操作:**

- 每次查询都优先从redis进行查询，若无数据，则前往数据库查询（@Cacheable注解）;

- 返回数据库数据的同时，查询结果存入redis ;

  

**更新删除操作:**

- 针对插入、更新，**直接操作数据库**，每次均会同步到redis，注意是删除缓存（@CachePut注解）
  - 为什么同步更新缓存时是删除 而不是 更新缓存 ？ 
    - 比如新线程最后更新了数据，但在它之前更新的旧线程因为网络问题滞后了，最后缓存更新过去地就是 旧的数据
  - 更新数据时异步缓存删除，此时用户读取到另一台服务器旧数据怎么办？
    - 对于访问量不大的情况下，这类脏读发生的概率不大 ；
    - 优化：更新与删除操作，每次先删除缓存再前往操作数据库；
  - 更新数据时 缓存删除 失败怎么办？
    - **捕捉 删除失败异常，放入消息队列中进行 删除重试** —— ack回执方式保证删除成功

    - 利用数据库binlog（存储了数据库更新日志），利用阿里的canal实时订阅数据库操作的binlog，同步到消息队列中，之后利用ack机制确保 完成删除缓存

      

**逻辑2 :**

- 对于变化不大的数据，比如首页课程信息等，可以采用定期更新缓存的方式 ;
- 为了避免更新缓存操作同时生效，还可**随机设置key的过期时间** ;

**前提 :**

- 在启动类上添加@EnableCaching注解

**补充 :** memcache也可用于缓存



### Redis缓存雪崩 / 击穿

**缓存雪崩** : 大量缓存key在某一刻集中失效，外加巨大的QPS，大量请求直接冲击数据库 ;

**缓存击穿 :** 某一时刻大量请求涌来，而缓存中没有这一数据（或者因为对数据的请求QPS太大，导致缓存失效），直接冲击数据库，宕机，一重启就宕机一重启就宕机 ;

**解决办法 :** 

- 多级缓存、主从架构、哨兵机制
- 备份 + 消息队列 + 加锁 （注意不能加synchronized互斥锁 -> 读操作也加锁，疯了！）



### 数据结构-跳表(skiplist)



#### 概念

**关键** ：

- 跳表结合了链表和二分查找的思想
- 由原始链表和一些通过“跳跃”生成的链表组成
- 第0层是原始链表，越上层“跳跃”的越高，元素越少
- 上层链表是下层链表的子序列
- 查找时从顶层向下，不断缩小搜索范围



**例如：**

当我们想要查询一个数据的时候，先查上层的链表，就很容易知道数据落在**哪个范围**，然后**跳到下一个层级里进行查询。**这样逐级往下，就把搜索范围一下子缩小了一大半。 

![640 (3)](E:\doc_repo\002-应用\images\640 (3).png)



#### **B+树 vs 跳表**

> 查询操作

**B+树**是多叉树结构，每个结点都是一个16k的数据页，能存放较多索引信息，所以**扇出很高**。**三层**左右就可以存储`2kw`左右的数据。也就是说查询一次数据，如果这些数据页都在磁盘里，那么最多需要查询**三次磁盘IO**。

**跳表**是链表结构，一条数据一个结点，如果最底层要存放`2kw`数据，且每次查询都要能达到**二分查找**的效果，`2kw`大概在`2的24次方`左右，所以，跳表大概高度在**24层**左右。最坏情况下，这24层数据会分散在不同的数据页里，也即是查一次数据会经历**24次磁盘IO**。

因此存放同样量级的数据，B+树的高度比跳表的要少，如果放在mysql数据库上来说，就是**磁盘IO次数更少，因此，B+树查询更快**。

> 写入操作

而针对**写操作**，B+树需要拆分合并索引数据页，跳表则独立插入，并根据随机函数确定层数，没有旋转和维持平衡的开销，因此，**跳表的写入性能会比B+树要好。**

其实，mysql的**存储引擎是可以换的**，以前是`MyIsam`，后来才有的`Innodb`，它们底层索引用的都是**B+树**。也就是说，你完全可以造一个索引为跳表的存储引擎装到mysql里。事实上，`facebook`造了个`rocksDB`的存储引擎，里面就用了**跳表**。直接说结论，它的**写入性能**确实是比innodb要好，但**读性能**确实比innodb要差不少。



#### 为什么Redis选用跳表

redis 是纯纯的内存数据库。

进行读写数据都是操作内存，跟磁盘没啥关系，因此也**不存在磁盘IO**了，所以层高就不再是跳表的劣势了。

并且前面也提到B+树是有一系列合并拆分操作的，换成红黑树或者其他AVL树的话也是各种旋转，目的也是**为了保持树的平衡**。

而跳表插入数据时，只需要随机一下，就知道自己要不要往上加索引，根本不用考虑前后结点的感受，也就**少了旋转平衡的开销**。





### Redis事务



| 命令      | 功能描述                                                     |
| --------- | ------------------------------------------------------------ |
| MULTI     | **事务开始的命令**，执行该命令后，后面执行的对Redis数据类型的**操作命令都会顺序的放进队列中**，等待执行EXEC命令后队列中的命令才会被执行 |
| DISCARD   | **放弃执行队列中的命令**，你可以理解为Mysql的回滚操作，**并且将当前的状态从事务状态改为非事务状态**。 |
| EXEC      | 执行该命令后**表示顺序执行队列中的命令**，执行完后并将结果显示在客户端，**将当前状态从事务状态改为非事务状态**。若是执行该命令之前有key被执行WATCH命令并且又被其它客户端修改，那么就会放弃执行队列中的所有命令，在客户端显示报错信息，若是没有修改就会执行队列中的所有命令。 |
| WATCH key | 表示指定监视某个key，**该命令只能在MULTI命令之前执行**，如果监视的key被其他客户端修改，**EXEC将会放弃执行队列中的所有命令** |
| UNWATCH   | **取消监视之前通过WATCH 命令监视的key**，通过执行EXEC 、DISCARD 两个命令之前监视的key也会被取消监视 |



### Redis分布式锁

**为什么需要分布式锁 ?**

在单体的应用开发场景中，在多线程的环境下，涉及并发同步的时候，为了保证一个代码块在同一时间只能由一个线程访问，我们一般可以**使用synchronized语法和ReetrantLock去保证，这实际上是本地锁的方式**。

也就是说，在同一个JVM内部，大家往往采用synchronized或者Lock的方式来解决多线程间的安全问题。但**在分布式集群工作的开发场景中**，在JVM之间，那么就需要一种更加高级的锁机制，来处理种跨JVM进程之间的线程安全问题
![20210505213026273](E:\doc_repo\002-应用\images\20210505213026273.png)



**用途** : 实现一个简单的秒杀系统的库存扣减 

**逻辑** : 在线程A通过setnx尝试去获取到抢购对象produce对象的锁，若是获取成功旧会返回1，获取不成功，说明当前对象的锁已经被其它线程锁持有 ; 获取锁成功后并设置key的生存时间，能够有效的防止出现死锁，最后就是通过`del`来实现删除key，这样其它的线程就也可以获取到这个对象的锁。



**分布式锁指令** : 主要依靠`setnx、getset、expire、del`这四个命令来实现。

1. `setnx`：命令表示如果key不存在，就会执行set命令，若是key已经存在，不会执行任何操作 ;
2. `getset`：将key设置为给定的value值，并返回原来的旧value值，若是key不存在就会返回返回nil ; 
3. `expire`：设置key生存时间，当当前时间超出了给定的时间，就会自动删除key ;
4. `del`：删除key，它可以删除多个key，语法如下：`DEL key [key …]`，若是key不存在直接忽略 ;



```java
public void redis(Produce produce) {
        long timeout= 10000L; // 超时时间
    
    	Long result= RedisUtil.setnx(produce.getId(), 
                                     String.valueOf(System.currentTimeMillis() + timeout));
    
        if (result!= null && result.intValue() == 1) { // 返回1表示成功获取到锁
         RedisUtil.expire(produce.getId(), 10);  // 有效期为10秒，防止死锁
         //执行业务操作
         ......
         //执行完业务后，释放锁
         RedisUtil.del(produce.getId());
        } else {
           System.println.out("没有获取到锁")
        }
    }
```



**潜在问题** : 执行完setnx成功后设置生存时间不生效，此时服务器宕机，那么key就会一直存在Redis中, 一旦出现了释放锁失败，或者没有手工释放，那么这个锁永远被占用，其他线程永远也抢不到锁, 所以, 需要**保障setnx和expire两个操作的原子性**，要么全部执行，要么全部不执行，二者不能分开。 

**解决办法** : 

1. 使用set的命令时，同时设置过期时间，不再单独使用 expire命令, 如 :  set test "111" EX 100 NX

   > set 命令的完整格式： 
   >
   > set key value \[EX seconds]\[PX milliseconds] [NX|XX] 
   >
   > 
   >
   > EX seconds：设置失效时长，单位秒 
   >
   > PX milliseconds：设置失效时长，单位毫秒 
   >
   > NX：key不存在时设置value，成功返回OK，失败返回(nil) 
   >
   > XX：key存在时设置value，成功返回OK，失败返回(nil) 

2. 通过**定时任务检查是否有设置生存时间**，没有的话都会统一进行设置生存时间 ; 

3. 还有比较好的解决方案, 即在上方逻辑中添加, 没有获取到锁则再次进行key的生存时间操作 ;



最后, Redis实现分布式锁，还可以使用`Redisson`来实现, 开箱即用

**关键** : Redisson分布式锁的框架主要的学习分为下面的5个点

1. 加锁机制
2. 解锁机制
3. 生存时间延长机制
4. 可重入加锁机制
5. 锁释放机制



# 消息队列

### 消息队列基本步骤是什么？

1.创建连接工厂 -> 获取新连接 -> 创建信道 ;

2.队列声明（消费者与发布者均可完成，先到先建立），queueDeclare方法参数 :

- 队列名 ;
- 是否为持久化队列（是否将队列信息存储到磁盘） ;
- 是否为独占队列 ;
- 若无消费者，是否将自动删除 ;
- 队列其他参数 ;

3.发布动作basicPublish()参数 :

- 交换机名 ;
- 队列名称 ;
- 其他参数 : 键值对形式，如` MessageProperties.PERSISTENT_BASIC ` ;
- 消息内容，若为String，应转化为bytes[] ;

4.消费动作basicConsume() :

- 队列名 ;
- **是否开启自动确认acknowlage（是否需要消费回执）** ;
- 消息处理回调对象 ;
- 取消处理回调对象 ;



**注意 :** 消息的生产者不一定有队列，但消费者必然有队列，可能是生产者直接发到队列，也有可能是 发到交换机，前提是 消费者的队列已和交换机绑定



### 交换机有哪几种？

**fanoutExchange :** 广播模式交换，接收到生产者消息后即可播报，无论是否存在消费者 ;

**directExchange :** 订阅模式，消费者根据 队列的路由键 消费对应消息，队列可与多个key绑定 ;

**TopicExchange :** 主题模式，队列路由键具有特殊格式，如\*.\*.rabbit、lazy.# 可匹配多种键 ;





### 发布-订阅模式的基本步骤？

**生产者 :** 



1.声明交换机channel.exchangeDeclare() ;

- 交换机名
- 交换机类型，如BuiltinExchangeType.FANOUT 或 "fanout" ;



2.发布动作basicPublish()参数 :

- 交换机名 ;
- 队列名称（一般设为空字符串""——发布订阅模式下生产者直接向交换机发布消息，不经过队列，况且生产者一旦声明交换机，第二个参数则为 路由键） ;
- 其他参数 : 键值对形式，如` MessageProperties.PERSISTENT_BASIC ` ;
- 消息内容，若为String，应转化为bytes[] ;



**消费者 :**

1.声明队列channel.queueDeclare();  

2.声明交换机channel.exchangeDeclare() :

- 交换机名
- 交换机类型，如BuiltinExchangeType.FANOUT 或 "fanout" ;

3.队列与交换机绑定channel.queueBind(队列名，交换机名) ;

4.消费动作basicConsume() 。



### 主题模式的基本步骤？



**生产者 :** 

1.声明交换机channel.exchangeDeclare("xxx", BuiltinExchangeType.DIRECT) ;

2.发布动作basicPublish()参数：交换机名、**路由键**、消息其他属性、消息内容 ；



**消费者 ：**

1.声明队列 + 声明交换机 ；

2.关键词绑定 ： channel.queueBind(队列名、交换机名、**路由键**) ；

3.消费动作basicConsume()。



### SpringBoot整合RabbitMQ注解

**依赖 :**

```pom
<dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

**主要注解包括 :**

- 简单、工作模式不涉及交换机的创建，因此在配置类中里用@Bean显示的创建队列 :

```java
	@Bean
    public Queue taskQueue() {
        return new Queue("task_queue"); //此处只给队列名，其他参数是下面的默认值
        // return new Queue("task_queue",true,false,false);
    }
```

- 广播、订阅、主题模式涉及交换机的创建，在配置类中里用@Bean显示的创建交换机 :

```java
	@Bean
    public TopicExchange logs() {
        // 广播模式为 FanoutExchange
        // 订阅模式为 DirectExchange
        return new TopicExchange("topic_logs", false, false);
        //交换机名、非持久，不自动删除
    }
```

- 生产者发布消息，需要**注入IoC容器内的AmqpTemplate，其内部方法convertAndSend为发布函数** :

```java
	@Autowired
    private AmqpTemplate amqpTemplate;

	amqpTemplate.convertAndSend(String name,String key，String message);
	//参数1: 队列名 或 交换机名
	//参数2: 路由键（可无）
	//参数3: 消息内容（无需如原生API中getBytes方法转换）
```

- 消费者消费消息，可在 类上加注解@RabbitListener(queues = "xxx") + 方法上@RabbitHandler ; 或者直接在方法上 @RabbitListener(queues = "xxx")

- 对于 广播、订阅、主题模式，在配置中创建了交换机，还必须在消费端进行**消费队列与交换机的绑定**，具体实现为配置 @RabbitListener的bindings参数

  -  @RabbitListener的bindings参数 = @QueueBinding注解 :
    - value = @Queue(name="xxx", durable=false, exclusif=true, autoDelete=true)
    - exchange = @Exchange(name = "logs", declare = "false")
    - key = {"\*.\*.rabbit", "lazy.#"}

  ```java
  	@RabbitListener(bindings = @QueueBinding(
              //队列,若@Queue不附带任何参数，则为随机命名队列，默认false, true, true
              value = @Queue,
              // declare = "false" 不创建交换机 -> 使用已存在的交换机
              exchange = @Exchange(name = "logs", declare = "false"), 
      		key = {"*.*.rabbit", "lazy.#"}
      										)
                     )
      public void receive1(String msg) {
          System.out.println("消费者1收到：" + msg);
      }
  ```

  

### 如何保证MQ中的消息不丢失？

**保证消息不丢失三步走** 
1 开启事务 (不推荐)
2 开启confirm (推荐)
3 开启RabbitMQ持久化 (交换机、队列、消息)
4 关闭RabbitMQ自动ack (改成手动)



**1.确保消息在生产端能发送成功 :**

方法一 : 通过事务实现，但性能消耗较大 ;

方法二 : 一般通过confirm实现 :

```java
channel.confirmSelect(); // 将信道设置成confirm模式。

channel.waitForConfirms();
等待发送消息的确认消息，如果发送成功，则返回ture，如果发送失败，则返回false，比如:

if (channel.waitForConfirms()) {
            System.out.println("send message success");
        } else {
            System.out.println("send message failed");
        }
```



**2.队列持久化到磁盘 :** 在声明队列时开启，queueDeclare()第二个参数 ;



**3.确保消息在消费端能被消费 :** 

- basicConsume()方法中的第二个参数ack设为false（取消-消息送到消费端即自动确认为收到） ;
- 在**消息处理回调对象中，业务操作完成后，发送回执**: channel.basicAck(message.getEnvelope().getDeliveryTag(), false); 
- 设置消息消费的Quantity of services（QOS），每次接收几条消息 ` channel.basicQos(n)`



### MQ工作模式和交换机有哪些？

**模式 :** 

简单模式（1对1）、工作模式（1对多）、发布订阅模式、主题模式

**交换机 :** 

direct（默认）、fanout、topic、header（罕用）

生产者只能向交换机(Exchange)发送消息 : 交换机一边接收来自生产者的消息，另一边将消息推送到队列。交换器必须确切地知道如何处理它接收到的消息。



### 如何解决消息的幂等性问题？

**概念：**计算机科学中，多次请求所产生的影响与一次请求执行的影响效果相同，在消息队列中，主要指的就是同一条消息不能被重复消费；再比如注册时的form表单，用户点击两次提交是否会导致插入两条注册数据等

**例子：**开发一个转账功能，假设我们调用下游接口**超时**了，可能有三种情况：

- 一般情况下，**超时**可能是**网络传输丢包**的问题
- 请求时没送到
- 请求到了，**返回结果却丢**了。 

**解决方案 :** 每个消息用一个唯一标识来区分，消费前先判断标识有没有被消费过，若已消费过，则直接ACK



**方案1：Token机制实现**

![640](E:\doc_repo\002-应用\images\640.png)

**具体流程步骤：**

1. 客户端会先发送一个请求去获取 token，服务端会生成一个全局唯一的 ID 作为 token 保存在 redis 中，同时把这个 ID 返回给客户端
2. 客户端第二次调用业务请求的时候必须携带这个 token
3. 服务端会校验这个 token，如果校验成功，则执行业务，并删除 redis 中的 token
4. 如果校验失败，说明 redis 中已经没有对应的 token，则表示重复操作，直接返回指定的结果给客户端



**方案2：基于 redis 实现** 

![640 (1)](E:\doc_repo\002-应用\images\640 (1).png)

**具体流程步骤：**

1. 客户端先请求服务端，会拿到一个能代表这次请求业务的唯一字段
2. 将该字段以 SETNX 的方式存入 redis 中，并根据业务设置相应的超时时间
3. 如果设置成功，证明这是第一次请求，则执行后续的业务逻辑
4. 如果设置失败，则代表已经执行过当前请求，直接返回





### RabbitMQ如何保证消息的顺序性

将消息放入同一个交换机，交给同一个队列，这个队列只有一个消费者，消费者只允许同时开启一个线程



### RabbitMQ消息重试机制

消费者在消费消息的时候，如果消费者业务逻辑出现程序异常，这时候应该如何处理？
答案：使用消息重试机制(SpringBoot默认3次消息重试机制)




### MQ应用场景

1.用户完成注册后，向消息队列追加消息——异步发送注册成功邮件 ;

2.异步生成积分、生成用户操作日志 ;



# 全文检索引擎

### 中文一般使用什么ES分词器？

- ik分词器，其中包含两种分词策略：ik_max_word 、ik_smart 
- 如果不使用ik这类第三方分词器，可以使用ES原生的ngram或edge-ngram分词器，默认ngram分词器最小字符数为1，步长为2，"Quick Fox"将被分为Q, Qu, u, ui, i, ic, c, ck, k, "k ", " ", " F", F, Fo, o, ox, x —— 即空格也会被考虑在内，若不想考虑空格，可设置分词器中的token_chars属性（包括letter、digit、whitespace、punctuation、symbol等）

```json
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 3,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}

// 分词结果为：Qui, uic, ick, Fox, oxe, xes 
```







### 索引index VS 分片shard VS 副本replicas

索引用来存储我们要搜索的数据，以倒排索引结构进行存储。例如，要搜索商品数据，可以创建一个商品数据的索引，其中存储着所有商品的数据；  

当索引中存储了大量数据时，大量的磁盘io操作会降低整体搜索新能，这时需要对数据进行分片存储。在一个索引中存储大量数据会造成性能下降，这时可以对数据进行分片存储。每个节点上都创建一个索引分片，把数据分散存放到多个节点的索引分片上，减少每个分片的数据量来提高io性能：每个分片都是一个**独立的索引**，数据分散存放在多个分片中，也就是说，每个分片中存储的都是不同的数据。搜索时会**同时搜索多个分片**，并将搜索结果进行汇总。 

如果一个节点宕机分片不可用，则会造成**部分数据无法搜索**： 对分片创建多个副本，那么即使一个节点宕机，其他节点中的副本分片还可以继续工作，不会造成数据不可用



### ES有那些数据类型？

- 数字类型：
  - byte、short、integer、long
  - float、double
  - unsigned_long
- 字符串类型：
  - text ： 会进行分词
  - keyword ： 不会进行分词，适用于email、主机地址、邮编等
- 日期和时间类型：
  - date



### 如何对搜索结果进行高亮显示？

- 原生ES有特殊的语法，比如highlight属性可以给搜索到的内容添加JavaScript标签；
- Springboot则有专门的注解；



### 了解solr吗？





### 如何进行数据库和ES的同步？





# Eureka

### fetch-registry和register-with-eureka参数分别代表什么?

register-with-eureka 表示是否将自己注册到Eureka Server

fetch-registry 表示是否从Eureka Server获取注册的服务信息



# 分布式事务

### SEATA-AT模式的几个关键角色是什么？

- 事务协调器TC
- 事务管理器TM
- 资源管理器RM

![20200726105428261](E:\doc_repo\002-应用\images\20200726105428261.png)





### SEATA是如何实现数据回滚的?

当你要更新一条记录的时候，系统会先根据这条记录原本的内容生成一个回滚日志存入 undo log 表中，将来要回滚的话，就根据 undo log 中的记录去更新数据（反向补偿），将来要是不回滚的话，就删除 undo log 中的记录。 



### file.conf和registry.conf文件分别配置什么信息?

一、registry.conf 主要配置 Seata 的注册中心，支持file 、nacos 、eureka、redis、zk、consul、etcd3、sofa几种形式 ;

二、file.conf 中配置 TC 的存储模式，TC 的存储模式有三种：

- file：适合单机模式，全局事务会话信息在内存中读写，并持久化本地文件 root.data，性能较高。
- db：适合集群模式，全局事务会话信息通过 db 共享，相对性能差点。
- redis：适合集群模式，全局事务会话信息通过 redis 共享，相对性能好点，但是要注意，redis 模式在 Seata-Server 1.3 及以上版本支持，性能较高，不过存在事务信息丢失的风险，所以需要开发者提前配置适合当前场景的 redis 持久化配置。

**注意 :** db数据库模式需要提前运行数据库脚本



### 分布式事务涉及到的数据库有什么共同点？

都必须有一个undo_log表记录更改前后的数据 :

```sql
CREATE TABLE `undo_log` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `branch_id` bigint(20) NOT NULL,
  `xid` varchar(100) NOT NULL,
  `context` varchar(128) NOT NULL,
  `rollback_info` longblob NOT NULL,
  `log_status` int(11) NOT NULL,
  `log_created` datetime NOT NULL,
  `log_modified` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
```





#AOP

### AOP的关键概念有哪些？

**切入点pointcut：**定义切面的目标方法，可以是基于注解、包路径下某方法、方法返回值类型等等

**通知advice：**前置通知、后置通知、环绕通知、返回后通知、抛出异常通知

**连接点joinpoint：**目标方法

**切面aspect：**切入点+通知



### 项目中对AOP的应用

MVC下面的 @RestControllerAdvice

声名式事务管理

手动完成 ： 日志记录



# 微服务

### 为什么需要分布式？微服务？

- 归根到底，最初需要分布式，目的在于提高响应性能，中国人口多经济增长快，各个模块分开部署在各自的服务器意味着响应性能的大幅提升（独占物理机的硬件资源，CPU内存等等）；
- 模块与模块之间解耦、更加便于维护;
- 提高整体的稳定性，在大型项目里，不同的微服务一般会部署在不同的服务器，部分服务器的宕机不会影响到其他服务器；
- 正是因为分布式的需求，才衍生出了各种微服务板块，以进行服务治理，比如 **注册中心、配置中心、网关、限流熔断处理、远程调用负载均衡**等等。



### 了解微服务吗？使用过微服务？





### 网关的作用有哪些？

路由转发 、 过滤器



自定义过滤器类，继承ZuulFilter，重写方法:

filterType() : 定义过滤器类型，包括pre, routing, post, error ;

filterOrder() : 定义顺序，比如验证token，order为6，在前一步时，已经向请求上下文对象RequestContext放入了 serviceId ;

shouldFilter() : 是否需要过滤，比如获取请求上下文，获取ServiceId，之后:

`return "item-service".equals(serviceId); `

run() : 定义过滤逻辑



# 其他



### Base64是什么?

专门为文本数据设计的媒体 (比如XML文档) 说到底当然也是二进制的，但文本媒体通常**使用某些二进制值作为底层的控制字符**。 此外，文本媒体可能会拒绝某些二进制值作为非文本。Base64 编码将二进制数据编码为只能在文本媒体中解释为文本的值，并且没有 任何特殊字符 和 / 或 控制字符，因此数据也将在文本媒体中保存。

比如, 假设您想在 XML 文档中嵌入几个图像。 图像是二进制数据，而 XML 文档是文本。 但是 XML 不能处理嵌入的二进制数据。 你是怎么做到的？一种选择是以 base64 对图像进行编码，将二进制数据转换为 XML 可以处理的文本。

**Instead of:**

```
<images>
  <image name="Sally">{乱七八糟的二进制数据}</image>
  <image name="Bobby">{乱七八糟的二进制数据}</image>
</images>
```

**you do:**

```
<images>
  <image name="Sally" encoding="base64">j23894uaiAJSD3234kljasjkSD...</image>
  <image name="Bobby" encoding="base64">Ja3k23JKasil3452AsdfjlksKsasKD...</image>
</images>
```

此时, XML 解析器将能够正确解析 XML 文档并提取图像数据。



**Base64编码问题可以有多急人?**

首先, 不同三方包的加解码规则可能不同, 强烈建议用加码包下的解码代码, 比如用sun.misc.BASE64Encoder编码, 而用org.springframework.util.Base64Utils解码, 则会出现异常 ;

再则, 不同的平台应用, 对Base64的解读会有差异, 比如 :

> 背景：客户端调用服务端接口, 参数需要使用base64加密传输
>
> bug：接口端解析最后一个字符总是乱码乱码, 使用了解码器测试依然无效
>
> 解决：客户端上传的base64编码, 居然最后的+号，被请求过滤处理为空格
>
> 根源 : Base64和URL传参问题, 标准的Base64并不适合直接放在URL里传输，因为URL编码器会把标准Base64中的“/”和“+”字符变为形如“%XX”的形式，而这些“%”号在存入数据库时还需要再进行转换，因为ANSI SQL中已将“%”号用作通配符










